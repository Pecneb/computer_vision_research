%% LaTeX2e file `biblio.bib'
%% generated by the `filecontents' environment
%% from source `documentation_hun' on 2023/03/01.
%%
@INPROCEEDINGS{1238912,
  author={Papadimitriou, I. and Tomizuka, M.},
  booktitle={Proceedings of the 2003 American Control Conference, 2003.}, 
  title={Fast lane changing computations using polynomials}, 
  year={2003},
  volume={1},
  number={},
  pages={48-53 vol.1},
  doi={10.1109/ACC.2003.1238912}
}

@INPROCEEDINGS{6696982,
  author={Houenou, Adam and Bonnifait, Philippe and Cherfaoui, Véronique and Yao, Wen},
  booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={Vehicle trajectory prediction based on motion model and maneuver recognition}, 
  year={2013},
  volume={},
  number={},
  pages={4363-4369},
  doi={10.1109/IROS.2013.6696982}
}

@ARTICLE{8186191,
  author={Xie, Guotao and Gao, Hongbo and Qian, Lijun and Huang, Bin and Li, Keqiang and Wang, Jianqiang},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Vehicle Trajectory Prediction by Integrating Physics- and Maneuver-Based Approaches Using Interactive Multiple Models}, 
  year={2018},
  volume={65},
  number={},
  pages={5999-6008},
  doi={10.1109/TIE.2017.2782236}
}

@INPROCEEDINGS{8317943,
  author={Kim, ByeoungDo and Kang, Chang Mook and Kim, Jaekyum and Lee, Seung Hi and Chung, Chung Choo and Choi, Jun Won},
  booktitle={2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network}, 
  year={2017},
  volume={},
  number={},
  pages={399-404},
  doi={10.1109/ITSC.2017.8317943}
}

@article{wang2022yolov7,
  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2207.02696},
  year={2022}
}
@inproceedings{Wojke2017simple,
  title={Simple Online and Realtime Tracking with a Deep Association Metric},
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
  year={2017},
  pages={3645--3649},
  organization={IEEE},
  doi={10.1109/ICIP.2017.8296962}
}
@inproceedings{Wojke2018deep,
  title={Deep Cosine Metric Learning for Person Re-identification},
  author={Wojke, Nicolai and Bewley, Alex},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2018},
  pages={748--756},
  organization={IEEE},
  doi={10.1109/WACV.2018.00087}
}

@inproceedings{10.1109/ICPR.2010.764,
author = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
title = {The Balanced Accuracy and Its Posterior Distribution},
year = {2010},
isbn = {9780769541099},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPR.2010.764},
doi = {10.1109/ICPR.2010.764},
abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
booktitle = {Proceedings of the 2010 20th International Conference on Pattern Recognition},
pages = {3121–3124},
numpages = {4},
keywords = {bias, generalizability, class imbalance, classification performance},
series = {ICPR '10}
}

@incollection{PAUL2017177,
title = {Chapter 8 - Big Data collision analysis framework},
editor = {Anand Paul and Naveen Chilamkurti and Alfred Daniel and Seungmin Rho},
booktitle = {Intelligent Vehicular Networks and Communications},
publisher = {Elsevier},
pages = {177-184},
year = {2017},
isbn = {978-0-12-809266-8},
doi = {https://doi.org/10.1016/B978-0-12-809266-8.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092668000089},
author = {Anand Paul and Naveen Chilamkurti and Alfred Daniel and Seungmin Rho},
keywords = {data analytics, Big Data, privacy in vehicles, ITS, data acquisition}
}

@article{10.1371/journal.pone.0253868,
    doi = {10.1371/journal.pone.0253868},
    author = {Rossi, Luca AND Ajmar, Andrea AND Paolanti, Marina AND Pierdicca, Roberto},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Vehicle trajectory prediction and generation using LSTM models and GANs},
    year = {2021},
    month = {07},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pone.0253868},
    pages = {1-28},
    abstract = {Vehicles’ trajectory prediction is a topic with growing interest in recent years, as there are applications in several domains ranging from autonomous driving to traffic congestion prediction and urban planning. Predicting trajectories starting from Floating Car Data (FCD) is a complex task that comes with different challenges, namely Vehicle to Infrastructure (V2I) interaction, Vehicle to Vehicle (V2V) interaction, multimodality, and generalizability. These challenges, especially, have not been completely explored by state-of-the-art works. In particular, multimodality and generalizability have been neglected the most, and this work attempts to fill this gap by proposing and defining new datasets, metrics, and methods to help understand and predict vehicle trajectories. We propose and compare Deep Learning models based on Long Short-Term Memory and Generative Adversarial Network architectures; in particular, our GAN-3 model can be used to generate multiple predictions in multimodal scenarios. These approaches are evaluated with our newly proposed error metrics N-ADE and N-FDE, which normalize some biases in the standard Average Displacement Error (ADE) and Final Displacement Error (FDE) metrics. Experiments have been conducted using newly collected datasets in four large Italian cities (Rome, Milan, Naples, and Turin), considering different trajectory lengths to analyze error growth over a larger number of time-steps. The results prove that, although LSTM-based models are superior in unimodal scenarios, generative models perform best in those where the effects of multimodality are higher. Space-time and geographical analysis are performed, to prove the suitability of the proposed methodology for real cases and management services.},
    number = {7},

}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{Anguita2012TheI,
  title={The 'K' in K-fold Cross Validation},
  author={D. Anguita and Luca Ghelardoni and Alessandro Ghio and L. Oneto and Sandro Ridella},
  booktitle={The European Symposium on Artificial Neural Networks},
  year={2012}
}

@article{10.1145/304181.304187,
author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J\"{o}rg},
title = {OPTICS: Ordering Points to Identify the Clustering Structure},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304187},
doi = {10.1145/304181.304187},
abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {49–60},
numpages = {12},
keywords = {visualization, database mining, cluster analysis}
}

@inproceedings{10.1145/304182.304187,
author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J\"{o}rg},
title = {OPTICS: Ordering Points to Identify the Clustering Structure},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304187},
doi = {10.1145/304182.304187},
abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
keywords = {database mining, cluster analysis, visualization},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/235968.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: An Efficient Data Clustering Method for Very Large Databases},
year = {1996},
issue_date = {June 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/235968.233324},
doi = {10.1145/235968.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {103–114},
numpages = {12}
}

@inproceedings{10.1145/233269.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: An Efficient Data Clustering Method for Very Large Databases},
year = {1996},
isbn = {0897917944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/233269.233324},
doi = {10.1145/233269.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
booktitle = {Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data},
pages = {103–114},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {SIGMOD '96}
}

@inproceedings{10.5555/3001460.3001507,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {clustering algorithms, arbitrary shape of clusters, handling nlj4-275oise, efficiency on large spatial databases},
location = {Portland, Oregon},
series = {KDD'96}
}

@article{10.1145/3068335,
author = {Schubert, Erich and Sander, J\"{o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
title = {DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3068335},
doi = {10.1145/3068335},
abstract = {At SIGMOD 2015, an article was presented with the title “DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation” that won the conference’s best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.},
journal = {ACM Trans. Database Syst.},
month = {jul},
articleno = {19},
numpages = {21},
keywords = {density-based clustering, range-search complexity, DBSCAN}
}